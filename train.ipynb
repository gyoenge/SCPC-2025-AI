{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ab6d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration, BitsAndBytesConfig\n",
    "from transformers import T5ForConditionalGeneration\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, TaskType\n",
    "import torch\n",
    "\n",
    "base_model_id = \"Salesforce/blip2-flan-t5-xl\"\n",
    "\n",
    "# Processor\n",
    "processor = Blip2Processor.from_pretrained(base_model_id, use_fast=True)\n",
    "\n",
    "# Model\n",
    "model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "    base_model_id,\n",
    "    load_in_8bit=True,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "# 4bit 양자화된 모델에 LoRA 적용을 위한 준비 \n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA 설정 정의\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\"query\", \"key\", \"value\", \"dense\"],  # BLIP2 Q-Former only \n",
    "    lora_dropout=0.1,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM  # flan-t5 기반은 SEQ_2_SEQ\n",
    ")\n",
    "\n",
    "# LoRA 모델 생성 (원래 모델 위에 adapter layer 삽입)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54ab286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total Parameters: {total_params:,}\")\n",
    "    print(f\"Trainable Parameters: {trainable_params:,}\")\n",
    "\n",
    "count_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7df7607",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from datasets import Dataset\n",
    "from transformers import Blip2Processor\n",
    "\n",
    "# Load train dataset\n",
    "train_data_dir = \"./dataset/generated/\"\n",
    "df = pd.read_csv(os.path.join(train_data_dir, \"question_answer.csv\"))\n",
    "\n",
    "# Build prompt\n",
    "def build_prompt(row):\n",
    "    return (\n",
    "        \"USER: Based on the image, write a description and create a multiple-choice question with four options (A, B, C, D).\\n\"\n",
    "        \"Answer the question by selecting the best option from A, B, C, or D.\\n\"\n",
    "        \"Respond only with a single letter: A, B, C, or D.\\n\"\n",
    "        \"Follow this exact format:\\n\\n\"\n",
    "        f\"Question: {row['Question']}\\n\"\n",
    "        f\"A. {row['A']}\\n\"\n",
    "        f\"B. {row['B']}\\n\"\n",
    "        f\"C. {row['C']}\\n\"\n",
    "        f\"D. {row['D']}\\n\\n\"\n",
    "        \"Description:\\n\"\n",
    "        \"Answer:\\n\\n\"\n",
    "        \"ASSISTANT:\"\n",
    "    )\n",
    "\n",
    "df[\"prompt\"] = df.apply(build_prompt, axis=1)\n",
    "\n",
    "# 출력 텍스트 (Description + Answer) 만들기\n",
    "def build_target(row):\n",
    "    return (\n",
    "        f\"Description: {row['Description']}\\n\"\n",
    "        f\"Answer: {row['answer']}\"\n",
    "    )\n",
    "\n",
    "df[\"target\"] = df.apply(build_target, axis=1)\n",
    "\n",
    "# Dataset으로 변환\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# 전처리 함수 정의\n",
    "def preprocess(example):\n",
    "    image = Image.open(example[\"img_path\"]).convert(\"RGB\")\n",
    "    inputs = processor(\n",
    "        text=example[\"prompt\"],\n",
    "        images=image,\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=384,   # 128 to 256~384\n",
    "    )\n",
    "    labels = processor.tokenizer(\n",
    "        example[\"target\"],\n",
    "        return_tensors=\"pt\",\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "    ).input_ids\n",
    "    return {\n",
    "        \"input_ids\": inputs[\"input_ids\"][0].to(torch.long),\n",
    "        \"attention_mask\": inputs[\"attention_mask\"][0].to(torch.long),\n",
    "        \"pixel_values\": inputs[\"pixel_values\"][0].to(torch.float32),\n",
    "        \"labels\": torch.tensor(labels[0], dtype=torch.long),\n",
    "    }\n",
    "\n",
    "# 전처리 적용\n",
    "processed_dataset = dataset.map(preprocess, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b94d3fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6220375e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.column_names)\n",
    "print(processed_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ea7bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = processor.tokenizer.vocab_size\n",
    "\n",
    "def sanitize_labels(example):\n",
    "    example[\"labels\"] = [\n",
    "        token if 0 <= token < vocab_size else -100\n",
    "        for token in example[\"labels\"]\n",
    "    ]\n",
    "    return example\n",
    "\n",
    "processed_dataset = processed_dataset.map(sanitize_labels)\n",
    "\n",
    "def zero_to_ignore(example):\n",
    "    example[\"labels\"] = [-100 if token == 0 else token for token in example[\"labels\"]]\n",
    "    return example\n",
    "\n",
    "processed_dataset = processed_dataset.map(zero_to_ignore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd5aa940",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cast_attention_mask(example):\n",
    "    if \"attention_mask\" in example:\n",
    "        example[\"attention_mask\"] = torch.tensor(example[\"attention_mask\"]).to(torch.float32)\n",
    "    if \"decoder_attention_mask\" in example:\n",
    "        example[\"decoder_attention_mask\"] = torch.tensor(example[\"decoder_attention_mask\"]).to(torch.float32)\n",
    "    return example\n",
    "\n",
    "processed_dataset = processed_dataset.map(cast_attention_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955bdd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, **kwargs):\n",
    "        inputs.pop(\"num_items_in_batch\", None)\n",
    "\n",
    "        outputs = model(**inputs)\n",
    "        loss = outputs.loss\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75954949",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./model/finetuned-blip2-flan-t5-xl\",\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=5,\n",
    "    logging_steps=10,\n",
    "    save_steps=200,\n",
    "    learning_rate=5e-5,\n",
    "    save_total_limit=3,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=processed_dataset,\n",
    "    tokenizer=processor.tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a8f21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 만 저장\n",
    "trainer.save_model()  # 모델 저장\n",
    "processor.tokenizer.save_pretrained(training_args.output_dir)  # tokenizer 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541ed7a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
